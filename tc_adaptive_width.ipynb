{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import ObjectiveFunction as of\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import helper_funcs as hf\n",
    "from math import sqrt\n",
    "from scipy.stats import uniform, norm\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2polar(x):\n",
    "    x = np.array(x)\n",
    "    return np.arctan2(x[1], x[0])\n",
    "\n",
    "def powlaw_samp(x_min, alpha, size=1):\n",
    "    \"\"\"\n",
    "    Samples from powerlaw dist with min value x_min.\n",
    "    \"\"\"\n",
    "    r = np.random.random(size=size)\n",
    "    return  x_min * (1 - r) ** (1 / (1-alpha))\n",
    "\n",
    "    # https://stats.stackexchange.com/questions/173242/random-sample-from-power-law-distribution\n",
    "    # https://arxiv.org/pdf/0706.1062.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_TC2_Width(Optimizer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 func: float = required,\n",
    "                 lr: float = required,\n",
    "                 height: float = required,\n",
    "                 width: float = required,\n",
    "                 momentum: float = 0\n",
    "    ):\n",
    "\n",
    "        self.height = height\n",
    "        if height != 1:\n",
    "            print(\"Warning: given height is not compatible with a counting scheme.\")\n",
    "        self.width_denom = -0.5*(1/width)**2    # how large our count regions are\n",
    "        self.step_count = 0\n",
    "        self.alpha = 2.5\n",
    "\n",
    "        self.func = func\n",
    "        defaults = dict(lr=lr,\n",
    "                        momentum=momentum)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "    def _metric(self, pred):\n",
    "\n",
    "        if not self.state:\n",
    "            return pred\n",
    "        \n",
    "        history = self.state['history'][0:-1]\n",
    "        last_ph = self.state['history'][-1]\n",
    "        \n",
    "        Vbias = 0\n",
    "\n",
    "        for ph in history:\n",
    "            v = last_ph - ph\n",
    "            Vbias += torch.exp(self.width_denom * torch.dot(v, v.T))\n",
    "        \n",
    "        Vbias = self.height * Vbias\n",
    "\n",
    "        # update detachment of tensors\n",
    "        self.state['history'][-1] = self.state['history'][-1].detach().clone()\n",
    "\n",
    "        # Adapt alpha - no phase preference\n",
    "        self.adapt_alpha(Vbias, pred)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def adapt_alpha(self, Vbias, pred):\n",
    "\n",
    "        p = float(Vbias/self.step_count)\n",
    "        \n",
    "        if p > 1:\n",
    "            raise ValueError(\"\"\"Should not have this much bias. \n",
    "                            Probably irregular walker behaviour - check the trajectory plots.\"\"\")\n",
    "\n",
    "        self.alpha = 2.5 + p\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                \n",
    "                grad = p.grad\n",
    "                \n",
    "                if 'history' not in self.state:\n",
    "                    self.state['history'] = [p]\n",
    "                else:\n",
    "                    self.state['history'].append(p)\n",
    "\n",
    "                if 'momentum_buffer' not in self.state:\n",
    "                    self.state['momentum_buffer'] = grad.detach().clone()\n",
    "                else:\n",
    "                    self.state['momentum_buffer'].mul_(group['momentum']).add_(grad, alpha=1)\n",
    "                \n",
    "                mom_grad = self.state['momentum_buffer']\n",
    "                p.add_(mom_grad, alpha=-group['lr'])\n",
    "\n",
    "                # Levy Flight noise\n",
    "                levy_r = float(powlaw_samp(x_min=group['lr']*0.01, alpha=self.alpha)) * torch.norm(grad)\n",
    "                theta = float(uniform.rvs(loc=0, scale=2*np.pi))\n",
    "                dir = np.array([np.cos(theta), np.sin(theta)])\n",
    "                levy_noise = levy_r * torch.Tensor(dir)\n",
    "                p.add_(levy_noise, alpha=-group['lr'])\n",
    "                print(self.alpha, self.step_count)\n",
    "                \n",
    "                # Periodic Boundary Conditions (INBUILT TO OPTIMISER TO ENSURE COORDINATES ARE BOUNDED)\n",
    "                p = self.func.apply_period(p)\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b83da2cdb1b4b83aa002bd55dbeaed7667d61edf6884c4b1653ebe587a3eea0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
