{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "import adaptive_algos as aa\n",
    "import helper_funcs as hf\n",
    "import ObjectiveFunction as of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, opt_alg, start, bounds, opt_params, n_epochs, repeat=False):\n",
    "    m = model(start=start, bounds=bounds)\n",
    "    \n",
    "    if opt_alg is torch.optim.Adam or opt_alg is torch.optim.SGD:\n",
    "            opt_problem = of.OptimisationProblem(\n",
    "                m,\n",
    "                opt_alg(params=m.parameters(), **opt_params),\n",
    "                n_epochs = n_epochs\n",
    "            )\n",
    "    else:\n",
    "        opt_problem = of.OptimisationProblem(\n",
    "            m,\n",
    "            opt_alg(params=m.parameters(), func=m, **opt_params),\n",
    "            n_epochs = n_epochs\n",
    "        )\n",
    "\n",
    "    if repeat: \n",
    "        losses, params, preds = opt_problem.run(logs=False)\n",
    "    else:\n",
    "        losses, params, preds = opt_problem.run()\n",
    "\n",
    "    if not repeat:\n",
    "        fig1 = opt_problem.visualise((-bounds, bounds), (-bounds, bounds), 0.1, render=\"contour\")\n",
    "        fig2 = hf.create_density_plot(params)\n",
    "        if hasattr(opt_problem.opt, 'alpha_record'):\n",
    "            fig3 = px.scatter(opt_problem.opt.alpha_record)\n",
    "            return [fig1, fig2, fig3]\n",
    "        \n",
    "        else:\n",
    "            return [fig1, fig2]\n",
    "    else:\n",
    "        return params[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karls\\Documents\\GitHub\\honours\\ObjectiveFunction.py:115: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(aa)\n",
    "importlib.reload(of)\n",
    "lr = 0.05\n",
    "bounds = 10 \n",
    "height = 1.0\n",
    "width = bounds/20\n",
    "batch_size = 10\n",
    "max_iterations = 1000\n",
    "scale_annealer = lambda progress: 1 - progress\n",
    "# def scale_annealer(progress):\n",
    "#     if progress < 0.33:\n",
    "#         return 1\n",
    "#     elif progress < 0.66:\n",
    "#         return 0.8\n",
    "#     else:\n",
    "#         return 0.01\n",
    "\n",
    "opt_params={'lr': lr, 'height': height, 'width': width, 'n_epochs': max_iterations, \n",
    "            'scale_annealer': scale_annealer}\n",
    "figs = run_experiment(of.AlpineN1, aa.SGD_TC, start=[2.5,2], bounds=bounds,\n",
    "                      opt_params={'lr': lr, 'height': height, 'width': width, \n",
    "                      'n_epochs': max_iterations, 'scale_annealer': scale_annealer},\n",
    "                      n_epochs=max_iterations\n",
    "                      )\n",
    "figs.extend(run_experiment(of.Ackley, aa.SGD_TC, start=[2.5,2], bounds=bounds,\n",
    "                      opt_params={'lr': lr, 'height': height, 'width': width, \n",
    "                      'n_epochs': max_iterations, 'scale_annealer': scale_annealer},\n",
    "                      n_epochs=max_iterations\n",
    "                      )\n",
    "            )\n",
    "figs.extend(run_experiment(of.Rosenbrock, aa.SGD_TC, start=[2.5,2], bounds=bounds,\n",
    "                      opt_params={'lr': lr, 'height': height, 'width': width, \n",
    "                      'n_epochs': max_iterations, 'scale_annealer': scale_annealer},\n",
    "                      n_epochs=max_iterations\n",
    "                      )\n",
    "            )\n",
    "hf.figures_to_html(figs, 'SGD_TC_testfinal.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with SGD, momentum, ADAM, SGD_TC\n",
    "# percentage of runs finding global minimum, quantitiative comparisons\n",
    "# time complexity comparison\n",
    "# highlight the fact that it does not get trapped\n",
    "# compare escape dynamics of SGD_TC with metadynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f608a3fb994fc79a06874428c22b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importlib.reload(aa)\n",
    "importlib.reload(of)\n",
    "n_exps = 1000\n",
    "final_params = []\n",
    "max_iterations = 1000\n",
    "\n",
    "lr = 0.05\n",
    "bounds = 10 \n",
    "height = 1.0\n",
    "width = bounds/20\n",
    "batch_size = 10\n",
    "scale_annealer = lambda progress: 1 - progress\n",
    "opt_params={'lr': lr, 'height': height, 'width': width, 'n_epochs': max_iterations, \n",
    "            'scale_annealer': scale_annealer}\n",
    "\n",
    "for n in tqdm(range(n_exps)):\n",
    "    final_param = run_experiment(of.AlpineN1, aa.SGD_TC, start=[2.5,2], bounds=bounds,\n",
    "                      opt_params=opt_params, n_epochs=max_iterations, repeat=True\n",
    "                      )\n",
    "    final_params.append(final_param)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db98acc3e95f82b5e324d7be007bf550050863aec1d0387171646eb022377307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
